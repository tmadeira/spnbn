@phdthesis{Peharz2015,
author = {Peharz, Robert},
school = {Graz University of Technology},
title = {{Foundations of Sum-Product Networks for Probabilistic Modeling}},
url = {https://www.researchgate.net/profile/Robert{\_}Peharz/publication/273000973{\_}Foundations{\_}of{\_}Sum-Product{\_}Networks{\_}for{\_}Probabilistic{\_}Modeling/links/54f49ff00cf2f28c1362088b.pdf},
year = {2015}
}
@article{Lee2013,
abstract = {Sum–product networks (SPNs) are deep architectures that can learn and infer at low computational costs. The structure of SPNs is especially impor-tant for their performance; however, structure learning for SPNs has until now been introduced only for batch-type dataset. In this study, we propose a new on-line incremental structure learning method for SPNs. We note that SPNs can be represented by mixtures of basis distributions. Online learning of SPNs can be formulated as an online clustering problem, in which a local assigning instance corresponds to modifying the tree-structure of the SPN incrementally. In the method, the number of hidden units and even layers are evolved dynamically on incoming data. The experimental results show that the proposed method outper-forms the online version of the previous method. In addition, it achieves the performance of batch structure learning.},
author = {Lee, Sang-Woo and Heo, Min-Oh and Zhang, Byoung-Tak},
journal = {LNCS},
pages = {220--227},
title = {{Online Incremental Structure Learning of Sum–Product Networks}},
url = {https://bi.snu.ac.kr/Publications/Conferences/International/ICONIP2013{\_}SWLee.pdf},
volume = {8227},
year = {2013}
}
@article{Rashwan2016,
abstract = {Probabilistic graphical models provide a gen-eral and flexible framework for reasoning about complex dependencies in noisy do-mains with many variables. Among the var-ious types of probabilistic graphical mod-els, sum-product networks (SPNs) have re-cently generated some interest because ex-act inference can always be done in linear time with respect to the size of the network. This is particularly attractive since it means that learning an SPN from data always yields a tractable model for inference. However, existing parameter learning algorithms for SPNs operate in batch mode and do not scale easily to large datasets. In this work, we explore online algorithms to ensure that pa-rameter learning can also be done tractably with respect to the amount of data. More specifically, we propose a new Bayesian mo-ment matching (BMM) algorithm that oper-ates naturally in an online fashion and that can be easily distributed. We demonstrate the effectiveness and scalability of BMM in comparison to online extensions of gradient descent, exponentiated gradient and expecta-tion maximization on 20 classic benchmarks and 4 large scale datasets.},
author = {Rashwan, Abdullah and Zhao, Han and Poupart, Pascal},
title = {{Online and Distributed Bayesian Moment Matching for Parameter Learning in Sum-Product Networks}},
url = {http://proceedings.mlr.press/v51/rashwan16.pdf},
year = {2016}
}
@misc{Poupart2017,
author = {Poupart, Pascal},
title = {{Guest Lecture in STAT946 - Deep Learning (University of Waterloo) on October 17th}},
url = {https://www.youtube.com/watch?v=Nm0jNqOnQ2o},
urldate = {2018-05-23},
year = {2017}
}
@article{Hsu2017,
abstract = {Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network. As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice. This paper describes the first online structure learning technique for continuous SPNs with Gaussian leaves. We also introduce an accompanying new parameter learning technique.},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.3121v1},
author = {Hsu, Wilson and Kalra, Agastya and Poupart, Pascal},
eprint = {arXiv:1412.3121v1},
file = {:Users/tiago/Downloads/a911066366b705e95fc9f426e78e9ba1178e56bb.pdf:pdf},
journal = {Iclr},
pages = {1--10},
title = {{Online Structure Learning for Sum-product Networks with Gaussian Leaves}},
url = {https://openreview.net/pdf?id=S1QefL5ge},
year = {2017}
}
@article{Amer2016,
abstract = {This paper addresses detection and localization of human activities in videos. We focus on activities that may have variable spatiotemporal arrangements of parts, and numbers of actors. Such activities are represented by a Sum-Product Network (SPN). A product node in SPN represents a particular arrangement of parts, and a sum node represents alternative arrangements. The sums and products are hierarchically organized, and grounded onto space-time windows covering the video. The windows provide evidence about the activity classes based on the Counting Grid (CG) model of visual words. This evidence is propagated bottom-up and topdown to parse the SPN graph for the explanation of the video. The node connectivity and model parameters of SPN and CG are jointly learned under two settings, weakly supervised, and supervised. For evaluation, we use our new Volleyball dataset, along with the benchmark datasets VIRAT, UT- Interactions, KTH, and TRECVID MED 2011. Our video classification and activity localization are superior to those of the state of the art on these datasets.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Amer, Mohamed R. and Todorovic, Sinisa},
doi = {10.1109/TPAMI.2015.2465955},
eprint = {arXiv:1011.1669v3},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Activity Recognition,Hierarchical Models,Sum-Product Networks},
pmid = {25246403},
title = {{Sum product networks for activity recognition}},
year = {2016}
}
@inproceedings{Peharz2014,
abstract = {Sum-product networks (SPNs) are a recently proposed type of probabilistic graphical models allowing complex variable interactions while still granting efficient inference. In this paper we demonstrate the suitability of SPNs for modeling log-spectra of speech signals using the application of artificial bandwidth extension, i.e. artificially replacing the high-frequency content which is lost in telephone signals. We use SPNs as observation models in hidden Markov models (HMMs), which model the temporal evolution of log short-time spectra. Missing frequency bins are replaced by the SPNs using most-probable-explanation inference, where the state-dependent reconstructions are weighted with the HMM state posterior. According to subjective listening and objective evaluation, our system consistently and significantly improves the state of the art. {\textcopyright} 2014 IEEE.},
author = {Peharz, Robert and Kapeller, Georg and Mowlaee, Pejman and Pernkopf, Franz},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
doi = {10.1109/ICASSP.2014.6854292},
isbn = {9781479928927},
issn = {15206149},
keywords = {HMM,SPN,graphical models,speech bandwidth extension},
title = {{Modeling speech with sum-product networks: Application to bandwidth extension}},
year = {2014}
}
@article{Nie2014,
abstract = {This work presents novel algorithms for learning Bayesian network structures with bounded treewidth. Both exact and approximate methods are developed. The exact method combines mixed-integer linear programming formulations for structure learning and treewidth computation. The approximate method consists in uniformly sampling {\$}k{\$}-trees (maximal graphs of treewidth {\$}k{\$}), and subsequently selecting, exactly or approximately, the best structure whose moral graph is a subgraph of that {\$}k{\$}-tree. Some properties of these methods are discussed and proven. The approaches are empirically compared to each other and to a state-of-the-art method for learning bounded treewidth structures on a collection of public data sets with up to 100 variables. The experiments show that our exact algorithm outperforms the state of the art, and that the approximate approach is fairly accurate.},
archivePrefix = {arXiv},
arxivId = {1406.1411},
author = {Nie, Siqi and Maua, Denis Deratani and de Campos, Cassio Polpo and Ji, Qiang and Mau{\'{a}}, Denis Deratani and de Campos, Cassio Polpo and Ji, Qiang},
eprint = {1406.1411},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 27},
title = {{Advances in Learning Bayesian Networks of Bounded Treewidth}},
year = {2014}
}
@book{Koller2009,
abstract = {Most tasks require a person or an automated system to reasonto reach conclusions based on available information. The framework of probabilistic graphical models, presented in this book, provides a general approach for this task. The approach is model-based, allowing interpretable models to be constructed and then manipulated by reasoning algorithms. These models can also be learned automatically from data, allowing the approach to be used in cases where manually constructing a model is difficult or even impossible. Because uncertainty is an inescapable aspect of most real-world applications, the book focuses on probabilistic models, which make the uncertainty explicit and provide models that are more faithful to reality.Probabilistic Graphical Models discusses a variety of models, spanning Bayesian networks, undirected Markov networks, discrete and continuous models, and extensions to deal with dynamical systems and relational data. For each class of models, the text describes the three fundamental cornerstones: representation, inference, and learning, presenting both basic concepts and advanced techniques. Finally, the book considers the use of the proposed framework for causal reasoning and decision making under uncertainty.The main text in each chapter provides the detailed technical development of the key ideas. Most chapters also include boxes with additional material: skill boxes, which describe techniques; case study boxes, which discuss empirical cases related to the approach described in the text, including applications in computer vision, robotics, natural language understanding, and computational biology; and concept boxes, which present significant concepts drawn from the material in the chapter. Instructors (and readers) can group chapters in various combinations, from core topics to more technically advanced material, to suit their particular needs.Adaptive Computation and Machine Learning series},
author = {Koller, Daphne and Friedman, Nir},
booktitle = {Foundations},
doi = {10.1016/j.ccl.2010.07.006},
isbn = {0262013193},
issn = {15582264},
pmid = {20937442},
title = {{Probabilistic Graphical Models: Principles and Techniques}},
year = {2009}
}
@inproceedings{Bahar1993,
abstract = {In this paper we present theory and experiments on the algebraic$\backslash$ndecision diagrams (ADDs). These diagrams extend BDD's by allowing values$\backslash$nfrom an arbitrary finite domain to be associated with the terminal$\backslash$nnodes. We present a treatment founded in Boolean algebras and discuss$\backslash$nalgorithms and results in applications like matrix multiplication and$\backslash$nshortest path algorithms. Furthermore, we outline possible applications$\backslash$nof ADD's to logic synthesis, formal verification, and testing of digital$\backslash$nsystems},
author = {Bahar, R.I. and Frohm, E.A. and Gaona, C.M. and Hachtel, G.D. and Macii, E. and Pardo, A. and Somenzi, F.},
booktitle = {Proceedings of 1993 International Conference on Computer Aided Design (ICCAD)},
doi = {10.1109/ICCAD.1993.580054},
file = {:Users/tiago/Downloads/A{\_}1008699807402.pdf:pdf},
isbn = {0-8186-4490-7},
issn = {09259856},
pages = {188--191},
publisher = {IEEE Comput. Soc. Press},
title = {{Algebraic decision diagrams and their applications}},
url = {http://ieeexplore.ieee.org/document/580054/},
year = {1993}
}
@article{Cheng2014,
abstract = {Sum product networks (SPNs) are a new class of deep proba-bilistic models. They can contain multiple hidden layers while keeping their inference and training times tractable. An SPN consists of interleaving layers of sum nodes and product nodes. A sum node can be interpreted as a hidden variable, and a prod-uct node can be viewed as a feature capturing rich interactions among an SPN's inputs. We show that the ability of SPN to use hidden layers to model complex dependencies among words, and its tractable inference and learning times, make it a suitable framework for a language model. Even though SPNs have been applied to a variety of vision problems [1, 2], we are the first to use it for language modeling. Our empirical comparisons with six previous language models indicate that our SPN has superior performance.},
author = {Cheng, Wei-Chen and Kok, Stanley and Pham, Hoai Vu and {Leong Chieu}, Hai and Ming, Kian and Chai, A},
keywords = {Index Terms,deep learning,language models,probabilistic graphical models,sum-product networks},
title = {{Language Modeling with Sum-Product Networks}},
url = {http://spn.cs.washington.edu/papers/is14.pdf},
year = {2014}
}
@article{Zhao2015,
abstract = {In this paper, we establish some theoretical connections between Sum-Product Networks (SPNs) and Bayesian Networks (BNs). We prove that every SPN can be converted into a BN in linear time and space in terms of the network size. The key insight is to use Algebraic Decision Diagrams (ADDs) to compactly represent the local conditional probability distributions at each node in the resulting BN by exploiting context-specific independence (CSI). The generated BN has a simple directed bipartite graphical structure. We show that by applying the Variable Elimination algorithm (VE) to the generated BN with ADD representations, we can recover the original SPN where the SPN can be viewed as a history record or caching of the VE inference process. To help state the proof clearly, we introduce the notion of {\{}$\backslash$em normal{\}} SPN and present a theoretical analysis of the consistency and decomposability properties. We conclude the paper with some discussion of the implications of the proof and establish a connection between the depth of an SPN and a lower bound of the tree-width of its corresponding BN.},
archivePrefix = {arXiv},
arxivId = {1501.01239},
author = {Zhao, Han and Melibari, Mazen and Poupart, Pascal},
eprint = {1501.01239},
isbn = {9781510810587},
title = {{On the Relationship between Sum-Product Networks and Bayesian Networks}},
url = {http://proceedings.mlr.press/v37/zhaoc15.pdf http://arxiv.org/abs/1501.01239},
year = {2015}
}
@article{Poon2012,
abstract = {The key limiting factor in graphical model inference and learning is the complexity of the partition function. We thus ask the question: what are general conditions under which the partition function is tractable? The answer leads to a new kind of deep architecture, which we call sum-product networks (SPNs). SPNs are directed acyclic graphs with variables as leaves, sums and products as internal nodes, and weighted edges. We show that if an SPN is complete and consistent it represents the partition function and all marginals of some graphical model, and give semantics to its nodes. Essentially all tractable graphical models can be cast as SPNs, but SPNs are also strictly more general. We then propose learning algorithms for SPNs, based on backpropagation and EM. Experiments show that inference and learning with SPNs can be both faster and more accurate than with standard deep networks. For example, SPNs perform image completion better than state-of-the-art deep networks for this task. SPNs also have intriguing potential connections to the architecture of the cortex.},
archivePrefix = {arXiv},
arxivId = {1202.3732},
author = {Poon, Hoifung and Domingos, Pedro},
eprint = {1202.3732},
title = {{Sum-Product Networks: A New Deep Architecture}},
url = {https://arxiv.org/pdf/1202.3732.pdf http://arxiv.org/abs/1202.3732},
year = {2012}
}
@article{Moro2014,
abstract = {We propose a data mining (DM) approach to predict the success of telemarketing calls for selling bank long-term deposits. A Portuguese retail bank was addressed, with data collected from 2008 to 2013, thus including the effects of the recent financial crisis. We analyzed a large set of 150 features related with bank client, product and social-economic attributes. A semi-automatic feature selection was explored in the modeling phase, performed with the data prior to July 2012 and that allowed to select a reduced set of 22 features. We also compared four DM models: logistic regression, decision trees (DTs), neural network (NN) and support vector machine. Using two metrics, area of the receiver operating characteristic curve (AUC) and area of the LIFT cumulative curve (ALIFT), the four models were tested on an evaluation set, using the most recent data (after July 2012) and a rolling window scheme. The NN presented the best results (AUC = 0.8 and ALIFT = 0.7), allowing to reach 79{\%} of the subscribers by selecting the half better classified clients. Also, two knowledge extraction methods, a sensitivity analysis and a DT, were applied to the NN model and revealed several key attributes (e.g.; Euribor rate, direction of the call and bank agent experience). Such knowledge extraction confirmed the obtained model as credible and valuable for telemarketing campaign managers. {\textcopyright} 2014 Elsevier B.V.},
author = {Moro, S{\'{e}}rgio and Cortez, Paulo and Rita, Paulo},
doi = {10.1016/j.dss.2014.03.001},
file = {::},
isbn = {0167-9236},
issn = {01679236},
journal = {Decision Support Systems},
keywords = {Bank deposits,Classification,Neural networks,Savings,Telemarketing,Variable selection},
pages = {22--31},
title = {{A data-driven approach to predict the success of bank telemarketing}},
url = {https://ac.els-cdn.com/S016792361400061X/1-s2.0-S016792361400061X-main.pdf?{\_}tid=af6d1db7-c96b-49bf-8412-61c2eeea819b{\&}acdnat=1524570241{\_}342a0a93dadc2765d16775f1efb02bfd},
volume = {62},
year = {2014}
}
